# EnIt-Transformer-FromScratch ğŸ‡¬ğŸ‡§â¡ï¸ğŸ‡®ğŸ‡¹

A Transformer encoderâ€“decoder implemented **from scratch in PyTorch** for **English â†’ Italian neural machine translation**, inspired by the paper *Attention Is All You Need*.

This project does **not** rely on high-level libraries like HuggingFace Transformers for modeling â€” all core components (attention, encoder, decoder, masking, positional encoding) are manually implemented.

---

## ğŸš€ Features

- Transformer Encoderâ€“Decoder architecture from scratch
- Scaled Dot-Product Attention & Multi-Head Attention
- Positional Encoding
- Causal masking for autoregressive decoding
- Teacher forcing during training
- Label smoothing
- Word-level tokenization
- TensorBoard logging
- Dockerized training environment

---

## ğŸ§  Model Architecture

- **Encoder**
  - Multi-Head Self-Attention
  - Feed Forward Network
  - Residual Connections + Layer Normalization

- **Decoder**
  - Masked Multi-Head Self-Attention
  - Encoderâ€“Decoder Cross Attention
  - Feed Forward Network

---

## ğŸ“Š Dataset

- **OPUS Books Dataset**
- Language Pair: **English â†’ Italian**
- Loaded using HuggingFace `datasets`

---

## âš™ï¸ Installation (Local)

```bash
pip install -r requirements.txt
python main.py


